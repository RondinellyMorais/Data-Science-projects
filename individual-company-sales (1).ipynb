{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. <a id='Introduction'>Introduction "},{"metadata":{},"cell_type":"markdown","source":"![foto](https://theyellowcarcompany.com/wp-content/uploads/2019/04/The_Yellow_Car_Company_Sales_TYCC-930x550.jpg)"},{"metadata":{},"cell_type":"markdown","source":"### The \"Individual Company Sales\" dataset is a very interesting example of how we can use a variety of customer information to predict the likelihood that he will buy a specific product or not. The product in question is generic so that our analysis can theoretically be applied to any product"},{"metadata":{},"cell_type":"markdown","source":"### This dataset includes about 40,000 rows and 15 feature variables. Each row corresponds to a customer infomation, and includes the variables:\n\n### 1. flag: Whether the customer has bought the target product or not\n\n### 2. gender: Gender of the customer\n\n### 3. education: Education background of customer\n\n### 4. house_val: Value of the residence the customer lives in\n\n### 5. age: Age of the customer by group\n\n### 6. online: Whether the customer had online shopping experience or not\n\n### 7. customer_psy: Variable describing consumer psychology based on the area of residence\n\n### 8. marriage: Marriage status of the customer\n\n### 9. children: Whether the customer has children or not\n\n### 10. occupation: Career information of the customer\n\n### 11. mortgage: Housing Loan Information of customers\n\n### 12. house_own: Whether the customer owns a house or not\n\n### 13. region: Information on the area in which the customer are located\n\n### 14. car_prob: The probability that the customer will buy a new car(1 means the maximum possible）\n\n### 15. fam_income: Family income Information of the customer(A means the lowest, and L means the highest)"},{"metadata":{},"cell_type":"markdown","source":"# 2. <a id='importing'>Importing the necessary libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Disable warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Import plotting modules\n!pip install chart-studio\nimport seaborn as sns\nsns.set()\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker\nimport plotly.express as px\nfrom plotly.offline import iplot\nfrom matplotlib import rcParams\n\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n%matplotlib inline\n\nwarnings.filterwarnings(\"ignore\")\nimport plotly.figure_factory as ff\nfrom colorama import Fore, Back, Style \n\n# Import encoder library\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder \n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. <a id='reading'>Reading the dataset.csv"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data\ndf = pd.read_csv('../input/individual-company-sales-data/sales_data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Fore.BLUE + 'Data information ....................',Style.RESET_ALL)\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.columns:\n    print(i, df[i].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. <a id='basic'>Basic Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['gender'] = df.gender.replace('U', np.NaN)\ndf['age'] = df.age.replace('1_Unk', np.NaN)\ndf['child'] = df.child.replace('U', np.NaN)\ndf['child'] = df.child.replace('0', np.NaN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fraction of missing values\ndf.isnull().sum() / df.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the outliers in 'house_val'\nplt.figure(figsize = (12, 8))\nsns.boxplot(data= df, x = 'house_val')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using quantile method to eliminate outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the quantile method\nhi_q1 = df['house_val'].quantile(.25)\nhi_q3 =df['house_val'].quantile(.75)\niqr = hi_q3 - hi_q1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hi_up = hi_q3 + 1.5*iqr\nhi_down = hi_q1 - 1.5*iqr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0 = df[(df['house_val']> hi_down) & (df['house_val'] < hi_up)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show 'house_val' without outliers\nplt.figure(figsize=(12,8))\nsns.boxplot(data= df0, x = 'house_val')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning new dataset for encoder\ndff = df0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pie plot of house owner\nplt.figure(figsize =(7, 7))\ndf['house_owner'].value_counts().head(10).plot.pie(autopct='%1.1f%%')\n\n# Unsquish the pie.\nimport matplotlib.pyplot as plt\nplt.gca().set_aspect('equal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of null values in house owner\n(df.isnull().sum() / df.shape[0] * 100)['house_owner']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The most house owner are owner, than we can fill missing values with house owner attribute"},{"metadata":{"trusted":true},"cell_type":"code","source":"dff['house_owner'] = dff['house_owner'].fillna(df.mode()['house_owner'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pie plot of age\nplt.figure(figsize =(7, 7))\ndf['age'].value_counts().head(10).plot.pie(autopct='%1.1f%%')\n\n# Unsquish the pie.\nimport matplotlib.pyplot as plt\nplt.gca().set_aspect('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of null values in age\n(df.isnull().sum() / df.shape[0] * 100)['age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff = dff.dropna(subset=['age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pie plot of child\nplt.figure(figsize =(7, 7))\ndf['child'].value_counts().head(10).plot.pie(autopct='%1.1f%%')\n\n# Unsquish the pie.\nimport matplotlib.pyplot as plt\nplt.gca().set_aspect('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of null values in child\n(df.isnull().sum() / df.shape[0] * 100)['child']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We don't have dominant categories in 'child', then we can´t fill the missing values. therefore, it is reasonable to disregard the 'child' column."},{"metadata":{"trusted":true},"cell_type":"code","source":"dff = dff.drop('child', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pie plot marriage\nplt.figure(figsize =(7, 7))\ndf['marriage'].value_counts().head(10).plot.pie(autopct='%1.1f%%')\n\n# Unsquish the pie.\nimport matplotlib.pyplot as plt\nplt.gca().set_aspect('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of null values in marriage\n(df.isnull().sum() / df.shape[0] * 100)['marriage']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### More than 80% marriage are marriage, then we can fill missing values with marriage attribute"},{"metadata":{"trusted":true},"cell_type":"code","source":"dff['marriage'] = dff['marriage'].fillna(dff.mode()['marriage'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pie plot gender\nplt.figure(figsize =(7, 7))\ndf['gender'].value_counts().head(10).plot.pie(autopct='%1.1f%%')\n\n# Unsquish the pie.\nimport matplotlib.pyplot as plt\nplt.gca().set_aspect('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of null values in gender\n(df.isnull().sum() / df.shape[0] * 100)['gender']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pie plot education\nplt.figure(figsize =(7, 7))\ndf['education'].value_counts().head(10).plot.pie(autopct='%1.1f%%')\n\n# Unsquish the pie.\nimport matplotlib.pyplot as plt\nplt.gca().set_aspect('equal')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of null values in education\n(df.isnull().sum() / df.shape[0] * 100)['education']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since we have small amounts of missing values in the 'education' and 'gender' columns, then we simply drop them."},{"metadata":{"trusted":true},"cell_type":"code","source":"dff = dff.dropna(subset=['gender', 'education'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking data cleaning\ndff.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### No more missing values"},{"metadata":{},"cell_type":"markdown","source":"# 5. <a id='details'>Feature Engineering of dataset columns"},{"metadata":{},"cell_type":"markdown","source":"### We started converting the data set columns that are of the object type into numeric values"},{"metadata":{"trusted":true},"cell_type":"code","source":"dff.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Firts coverting  the hierarchy  columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting flag and online features to binary integer\ndff['flag'] = dff['flag'].apply(lambda value: 1 if value == 'Y' else 0)\ndff['online'] = dff['online'].apply(lambda value: 1 if value == 'Y' else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting education to integer\ndff['education'] = dff['education'].apply(lambda value: int(value[0]) + 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting age to integer\ndff['age'] = dff['age'].apply(lambda value: int(value[0]) - 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting mortgage to integer\ndff['mortgage'] = dff['mortgage'].apply(lambda value: int(value[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fam_income label dictionary\ndict_fam_income_label = {}\nfor i, char in enumerate(sorted(dff['fam_income'].unique().tolist())):\n    dict_fam_income_label[char] = i + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff['fam_income'] = dff['fam_income'].apply(lambda value: dict_fam_income_label[value])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, we deal of the columns with dummy variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_features = ['gender', 'customer_psy', 'occupation', 'house_owner', 'region', 'marriage']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_dummy(dff, i, drop_first=True):\n\n\n    return pd.concat([dff, pd.get_dummies(dff[i], prefix=i, drop_first=drop_first)], axis=1).drop(i, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting dummy features in numerical values\nfor i in dummy_features:\n    dff = apply_dummy(dff, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dff.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### All columns contain numerical values, but note that we have many more columns now, it is a price that we have to pay"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heatmap of correlation\nplt.figure(figsize=(14,14))\nsns.heatmap(dff.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Looking the heatmap of correlation we can see the most variables exhibit low positive and negative correlation. Remembering tha positive correlation can be definide like: if the value of one of the variables increases, the value of the other variable increases as well. In case negative correlation the value of one variable decreases with the other’s increasing and vice-versa."},{"metadata":{},"cell_type":"markdown","source":"# 6. <a id='details'> Using machine learning to predict heart disease"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataset into features and target\ny0 = dff[\"flag\"]\nx0 = dff.drop(\"flag\", axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the data into test data and training data\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x0, y0, test_size = 0.3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_list = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree Classifier\n\ndt_clf = DecisionTreeClassifier(max_leaf_nodes=10, random_state=30, criterion='entropy')\ndt_clf.fit(x_train, y_train)\ndt_pred = dt_clf.predict(x_test)\ndt_acc = dt_clf.score(x_test,y_test)\naccuracy_list.append(100*dt_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Fore.GREEN + \"Accuracy of Decision Tree Classifier is : \", \"{:.2f}%\".format(100* dt_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nplt.figure(figsize = (8, 8))\nmat = confusion_matrix(y_test, dt_pred)\nsns.heatmap(mat.T, square=True, annot=True,fmt=\"d\", cbar = False)\nplt.title(\"Decision Tree Clasifier - Confusion Matrix\")\nplt.xticks(range(2), [\"0\",\"1\"], fontsize=16)\nplt.yticks(range(2), [\"0\",\"1\"], fontsize=16)\nplt.xlabel(\"true label\")\nplt.ylabel(\"predicted label\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K Neighbors Classifier\n\nkn_clf = KNeighborsClassifier(n_neighbors=6)\nkn_clf.fit(x_train, y_train)\nkn_pred = kn_clf.predict(x_test)\nkn_acc = kn_clf.score(x_test,y_test)\naccuracy_list.append(100*kn_acc)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Fore.GREEN + \"Accuracy of K Neighbors Classifier is : \", \"{:.2f}%\".format(100* kn_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix of  K Neighbors Classifier\nfrom sklearn.metrics import confusion_matrix\nplt.figure(figsize = (8, 8))\nmat = confusion_matrix(y_test, kn_pred)\nsns.heatmap(mat.T, square=True, annot=True,fmt=\"d\", cbar = False)\nplt.xlabel(\"true label\")\nplt.ylabel(\"predicted label\")\nplt.title(\"K Neighbors Classifier - Confusion Matrix\")\nplt.xticks(range(2), [\"0\",\"1\"], fontsize=16)\nplt.yticks(range(2), [\"0\",\"1\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RandomForestClassifier\nr_clf = RandomForestClassifier(max_features=0.5, max_depth=15, random_state=1)\nr_clf.fit(x_train, y_train)\nr_pred = r_clf.predict(x_test)\nr_acc = r_clf.score(x_test,y_test)\naccuracy_list.append(100*r_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Fore.GREEN + \"Accuracy of Random Forest Classifier is : \", \"{:.2f}%\".format(100* r_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix of Random Forest Classifier \nfrom sklearn.metrics import confusion_matrix\nplt.figure(figsize = (8, 8))\nmat = confusion_matrix(y_test, r_pred)\nsns.heatmap(mat.T, square=True, annot=True,fmt=\"d\", cbar = False)\nplt.xlabel(\"true label\")\nplt.ylabel(\"predicted label\")\nplt.title(\"Random Forest Classifier - Confusion Matrix\")\nplt.xticks(range(2), [\"0\",\"1\"], fontsize=16)\nplt.yticks(range(2), [\"0\",\"1\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\ngradientboost_clf = GradientBoostingClassifier(max_depth=2, random_state=4)\ngradientboost_clf.fit(x_train,y_train)\ngradientboost_pred = gradientboost_clf.predict(x_test)\ngradientboost_acc = gradientboost_clf.score(x_test,y_test)\naccuracy_list.append(100*gradientboost_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Fore.GREEN + \"Accuracy of Gradient Boosting is : \", \"{:.2f}%\".format(100* gradientboost_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix of Gradient Boosting\nfrom sklearn.metrics import confusion_matrix\nplt.figure(figsize = (8, 8))\nmat = confusion_matrix(y_test, gradientboost_pred)\nsns.heatmap(mat.T, square=True, annot=True,fmt=\"d\", cbar = False)\nplt.xlabel(\"true label\")\nplt.ylabel(\"predicted label\")\nplt.title(\" Gradient Boosting - Confusion Matrix\")\nplt.xticks(range(2), [\"0\",\"1\"], fontsize=16)\nplt.yticks(range(2), [\"0\",\"1\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ngaussian_pred = gaussian.predict(x_test)\ngaussian_acc = gaussian.score(x_test,y_test)\naccuracy_list.append(100*gaussian_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Fore.GREEN + \"Accuracy of Gradient Boosting is : \", \"{:.2f}%\".format(100* gaussian_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix of GaussianNB\nfrom sklearn.metrics import confusion_matrix\nplt.figure(figsize = (8, 8))\nmat = confusion_matrix(y_test, gaussian_pred)\nsns.heatmap(mat.T, square=True, annot=True,fmt=\"d\", cbar = False)\nplt.xlabel(\"true label\")\nplt.ylabel(\"predicted label\")\nplt.title(\"GaussianNB - Confusion Matrix\")\nplt.xticks(range(2), [\"0\",\"1\"], fontsize=16)\nplt.yticks(range(2), [\"0\",\"1\"], fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list = ['DecisionTree', 'KNearestNeighbours', 'RandomForest', 'GradientBooster', 'GaussianNB']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=20,8\nsns.set_style('darkgrid')\nax = sns.barplot(x=model_list, y=accuracy_list, palette = \"vlag\", saturation =2.0)\nplt.xlabel('Classifier Models', fontsize = 20 )\nplt.ylabel('% of Accuracy', fontsize = 20)\nplt.title('Accuracy of different Classifier Models', fontsize = 20)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 12)\nfor i in ax.patches:\n    width, height = i.get_width(), i.get_height()\n    x, y = i.get_xy() \n    ax.annotate(f'{round(height,2)}%', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We use five machine learning algorithms to predict whether a customer is likely to buy a particular product or not based on various information about them. The best performing algorithms were GradientBooster and RandomForest with efficiency around 69%. Since the target variable represents a generic product so that we can apply our predictive models to any particular product we want to analyze. As the positive and negative correlations between the variables are not very large, it directly implies the model's prediction efficiency. We can conclude that depending on the correlations, we can obtain great prediction efficiency with the machine learning models."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}